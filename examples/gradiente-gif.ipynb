{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61c5a3a",
   "metadata": {},
   "source": [
    "# Optimización por Descenso de Gradiente con Animación\n",
    "\n",
    "**Ejemplo de minimización de f(x) = x² usando el método del gradiente descendente.**\n",
    "\n",
    "**Autor:** Dr. Fabián Astudillo Salinas  \n",
    "**Curso:** Fundamentos de Optimización\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Este notebook genera una **animación GIF** que muestra cómo el algoritmo de descenso de gradiente encuentra el mínimo de la función paso a paso.\n",
    "\n",
    "**Función objetivo:** $f(x) = x^2$  \n",
    "**Gradiente:** $\\nabla f(x) = 2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar librerías necesarias\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a001c2a",
   "metadata": {},
   "source": [
    "## 1. Definición de la función objetivo y su gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Función objetivo a minimizar\"\n",
    "f(x) = x^2\n",
    "\n",
    "\"Gradiente de f (derivada primera)\"\n",
    "∇f(x) = 2x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339c45e",
   "metadata": {},
   "source": [
    "## 2. Algoritmo de Descenso de Gradiente\n",
    "\n",
    "El algoritmo actualiza iterativamente la posición según:\n",
    "\n",
    "$$x_{nuevo} = x_{actual} - \\alpha \\cdot \\nabla f(x_{actual})$$\n",
    "\n",
    "donde $\\alpha$ es la tasa de aprendizaje (learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    gradient_descent(f, ∇f, x₀; α=0.1, max_iter=50, tol=1e-6)\n",
    "\n",
    "Minimiza una función usando el método del gradiente descendente.\n",
    "\n",
    "# Argumentos\n",
    "- `f`: función objetivo a minimizar\n",
    "- `∇f`: gradiente de la función objetivo\n",
    "- `x₀`: punto inicial\n",
    "- `α`: tasa de aprendizaje (learning rate)\n",
    "- `max_iter`: número máximo de iteraciones\n",
    "- `tol`: tolerancia para criterio de parada\n",
    "\n",
    "# Retorna\n",
    "- `x_history`: historial de puntos visitados\n",
    "- `x_opt`: punto óptimo encontrado\n",
    "- `f_opt`: valor de la función en el óptimo\n",
    "\"\"\"\n",
    "function gradient_descent(f, ∇f, x₀; α=0.1, max_iter=50, tol=1e-6)\n",
    "    x_current = x₀\n",
    "    x_history = [x_current]\n",
    "    \n",
    "    for i in 1:max_iter\n",
    "        gradient = ∇f(x_current)\n",
    "        x_new = x_current - α * gradient\n",
    "        push!(x_history, x_new)\n",
    "        \n",
    "        # Criterio de parada: cambio relativo pequeño\n",
    "        if abs(x_new - x_current) < tol\n",
    "            println(\"Convergencia alcanzada en iteración $i\")\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        x_current = x_new\n",
    "    end\n",
    "    \n",
    "    return x_history, x_current, f(x_current)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebbef9",
   "metadata": {},
   "source": [
    "## 3. Configuración de parámetros\n",
    "\n",
    "Ajusta estos valores para experimentar con diferentes configuraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros del algoritmo\n",
    "learning_rate = 0.1\n",
    "max_iterations = 50\n",
    "initial_point = 10.0\n",
    "tolerance = 1e-6\n",
    "\n",
    "println(\"=\"^60)\n",
    "println(\"OPTIMIZACIÓN POR DESCENSO DE GRADIENTE\")\n",
    "println(\"=\"^60)\n",
    "println(\"Función objetivo: f(x) = x²\")\n",
    "println(\"Punto inicial: x₀ = $initial_point\")\n",
    "println(\"Tasa de aprendizaje: α = $learning_rate\")\n",
    "println(\"Máximo de iteraciones: $max_iterations\")\n",
    "println(\"Tolerancia: $tolerance\")\n",
    "println(\"=\"^60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4bfcfe",
   "metadata": {},
   "source": [
    "## 4. Ejecutar el algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a83d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el algoritmo\n",
    "x_history, x_opt, f_opt = gradient_descent(\n",
    "    f, ∇f, initial_point;\n",
    "    α=learning_rate,\n",
    "    max_iter=max_iterations,\n",
    "    tol=tolerance\n",
    ")\n",
    "\n",
    "# Mostrar resultados\n",
    "println(\"\\nResultados:\")\n",
    "println(\"  Iteraciones realizadas: $(length(x_history) - 1)\")\n",
    "println(\"  Punto óptimo: x* = $x_opt\")\n",
    "println(\"  Valor mínimo: f(x*) = $f_opt\")\n",
    "println(\"  Error absoluto: |x* - 0| = $(abs(x_opt))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2ed64",
   "metadata": {},
   "source": [
    "## 5. Generar animación GIF\n",
    "\n",
    "Esta celda genera una animación que muestra el proceso de optimización frame por frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rango para graficar la función\n",
    "x_range = -12.0:0.1:12.0\n",
    "y_values = f.(x_range)\n",
    "\n",
    "println(\"\\nGenerando animación...\")\n",
    "\n",
    "# Crear animación frame por frame\n",
    "anim = @animate for i in 1:length(x_history)\n",
    "    # Crear gráfica base\n",
    "    p = plot(\n",
    "        x_range, y_values,\n",
    "        label=\"f(x) = x²\",\n",
    "        xlabel=\"x\",\n",
    "        ylabel=\"f(x)\",\n",
    "        title=\"Optimización por Descenso de Gradiente - Iteración $(i-1)\",\n",
    "        legend=:topright,\n",
    "        linewidth=2,\n",
    "        color=:blue,\n",
    "        size=(800, 600),\n",
    "        ylim=(-5, maximum(f.(x_history)) + 10)\n",
    "    )\n",
    "    \n",
    "    # Añadir línea del mínimo teórico\n",
    "    hline!(p, [0.0], label=\"Mínimo teórico\", linestyle=:dash, color=:black, alpha=0.3)\n",
    "    \n",
    "    # Marcar el punto inicial\n",
    "    scatter!(\n",
    "        p, [x_history[1]], [f(x_history[1])],\n",
    "        label=\"Inicio: x₀=$(round(x_history[1], digits=2))\",\n",
    "        markersize=8,\n",
    "        markercolor=:green,\n",
    "        markershape=:circle\n",
    "    )\n",
    "    \n",
    "    # Agregar el recorrido hasta la iteración actual\n",
    "    if i > 1\n",
    "        plot!(\n",
    "            p, x_history[1:i], f.(x_history[1:i]),\n",
    "            label=\"Trayectoria\",\n",
    "            linewidth=1.5,\n",
    "            color=:red,\n",
    "            alpha=0.5,\n",
    "            linestyle=:dash\n",
    "        )\n",
    "        \n",
    "        scatter!(\n",
    "            p, x_history[1:i-1], f.(x_history[1:i-1]),\n",
    "            label=\"Iteraciones previas\",\n",
    "            markersize=3,\n",
    "            markercolor=:red,\n",
    "            alpha=0.4\n",
    "        )\n",
    "    end\n",
    "    \n",
    "    # Marcar el punto actual con un marcador grande\n",
    "    scatter!(\n",
    "        p, [x_history[i]], [f(x_history[i])],\n",
    "        label=\"Actual: x=$(round(x_history[i], digits=4))\",\n",
    "        markersize=10,\n",
    "        markercolor=:gold,\n",
    "        markershape=:star5\n",
    "    )\n",
    "    \n",
    "    # Añadir información del gradiente\n",
    "    if i < length(x_history)\n",
    "        grad_val = ∇f(x_history[i])\n",
    "        annotate!(\n",
    "            p, x_history[i], f(x_history[i]) + 10,\n",
    "            text(\"∇f = $(round(grad_val, digits=3))\", :bottom, 8)\n",
    "        )\n",
    "    end\n",
    "end\n",
    "\n",
    "# Mostrar la animación en el notebook\n",
    "display(gif(anim, fps=3))\n",
    "\n",
    "# También guardar como archivo\n",
    "gif_filename = \"gradiente_optimizacion.gif\"\n",
    "gif(anim, gif_filename, fps=3)\n",
    "println(\"✓ Animación también guardada en '$gif_filename'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6932a9",
   "metadata": {},
   "source": [
    "## 6. Gráfica estática final\n",
    "\n",
    "Imagen con toda la trayectoria completa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear gráfica estática final\n",
    "p_final = plot(\n",
    "    x_range, y_values,\n",
    "    label=\"f(x) = x²\",\n",
    "    xlabel=\"x\",\n",
    "    ylabel=\"f(x)\",\n",
    "    title=\"Optimización por Descenso de Gradiente - Resultado Final\",\n",
    "    legend=:topright,\n",
    "    linewidth=2,\n",
    "    color=:blue,\n",
    "    size=(800, 600)\n",
    ")\n",
    "\n",
    "# Agregar el recorrido completo del algoritmo\n",
    "plot!(\n",
    "    p_final, x_history, f.(x_history),\n",
    "    label=\"Trayectoria completa\",\n",
    "    linewidth=1.5,\n",
    "    color=:red,\n",
    "    alpha=0.5,\n",
    "    linestyle=:dash\n",
    ")\n",
    "\n",
    "scatter!(\n",
    "    p_final, x_history, f.(x_history),\n",
    "    label=\"Iteraciones (n=$(length(x_history)-1))\",\n",
    "    markersize=4,\n",
    "    markercolor=:red,\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Marcar el punto inicial\n",
    "scatter!(\n",
    "    p_final, [initial_point], [f(initial_point)],\n",
    "    label=\"Inicio: x₀=$initial_point\",\n",
    "    markersize=8,\n",
    "    markercolor=:green,\n",
    "    markershape=:circle\n",
    ")\n",
    "\n",
    "# Marcar el punto final (óptimo)\n",
    "scatter!(\n",
    "    p_final, [x_opt], [f_opt],\n",
    "    label=\"Óptimo: x*=$(round(x_opt, digits=4))\",\n",
    "    markersize=10,\n",
    "    markercolor=:gold,\n",
    "    markershape=:star5\n",
    ")\n",
    "\n",
    "# Añadir línea del mínimo teórico\n",
    "hline!(p_final, [0.0], label=\"Mínimo teórico\", linestyle=:dash, color=:black, alpha=0.3)\n",
    "\n",
    "# Mostrar en el notebook\n",
    "display(p_final)\n",
    "\n",
    "# Guardar imagen estática\n",
    "savefig(p_final, \"gradiente_optimizacion.png\")\n",
    "println(\"✓ Gráfica estática guardada en 'gradiente_optimizacion.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca3c82",
   "metadata": {},
   "source": [
    "## Ejercicios propuestos\n",
    "\n",
    "1. **Experimenta con diferentes tasas de aprendizaje:**\n",
    "   - ¿Qué pasa si α = 0.01? ¿Y si α = 0.5?\n",
    "   - ¿Cuándo el algoritmo diverge?\n",
    "\n",
    "2. **Cambia el punto inicial:**\n",
    "   - Prueba con x₀ = -15 o x₀ = 0.5\n",
    "   - Observa cómo cambia la trayectoria\n",
    "\n",
    "3. **Modifica la función objetivo:**\n",
    "   - Prueba con f(x) = x⁴ - 3x² + x\n",
    "   - Calcula su gradiente: ∇f(x) = 4x³ - 6x + 1\n",
    "\n",
    "4. **Ajusta la velocidad de la animación:**\n",
    "   - Cambia el parámetro `fps` (frames por segundo) en la celda 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
